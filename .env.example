POSTGRES_USER=pineapple
POSTGRES_PASSWORD=pineapple
POSTGRES_DB=pineapple
POSTGRES_HOST=db
POSTGRES_PORT=5432
DATABASE_URL=postgresql+psycopg2://pineapple:pineapple@db:5432/pineapple
REDIS_URL=redis://redis:6379/0

# API key for protected endpoints (e.g., /subreddits/{name}/refresh)
# Generate a new one with: python -c "import secrets; print(secrets.token_urlsafe(32))"
API_KEY=your-api-key-here

# Timezone for all containers
TZ=Europe/Copenhagen

# Log level for scanner (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# How many API requests to allow per minute to avoid rate limiting
API_MAX_CALLS_MINUTE=8

# Seconds to spend refreshing metadata after each scan completes (before next scan)
# Example: set to 0 to continuously consider metadata stale and refresh every iteration
METADATA_REFRESH_SECONDS=0

# Start with metadata refresh on scanner startup (true/false)
SCAN_FOR_METADATA_FIRST=true

# Testing helper variables (to limit scope during tests)
# Maximum number of posts to process per source subreddit (e.g., 2 posts from nsfw411 AND 2 from wowthissubexists)
TEST_MAX_POSTS_PER_SUBREDDIT=

# How far back to initially scan posts from source subreddits.
# Posts older than this will be skipped entirely (not added to database).
# Reddit blocks comments on posts older than 180 days, so values > 180 may waste resources.
# Dev: 60-90 (2-3 months for testing), Prod: 180 (Reddit's comment archive limit)
POST_INITIAL_SCAN_DAYS=36500

# How far back to re-check existing posts for new/edited comments.
# Only posts already in the database will be re-scanned if within this window.
# Should be â‰¤ 180 days since Reddit blocks comments on older posts.
# Set to 0 to disable re-scanning. Empty = re-scan all existing posts (not recommended).
# Dev: 30-60 (1-2 months), Prod: 180 (maximum useful value)
POST_RESCAN_DAYS=36500

# Skip posts that were scanned within the last X hours (useful for container restarts)
# Set to 0 to disable this feature and always scan posts.
SKIP_RECENTLY_SCANNED_HOURS=0

# How many seconds to sleep between scan iterations (after metadata refresh completes)
SCAN_SLEEP_SECONDS=60

# How many seconds to spend rescanning posts from the DB each iteration.
# Set large value to rescan all posts during long-running runs (example purpose).
POST_RESCAN_DURATION=86400

# NOTE: Scan configuration is now managed in the database via these tables:
# - subreddit_scan_configs: which subreddits to scan and how (users, NSFW filter)
# - ignored_subreddits: subreddits to never record mentions for
# - ignored_users: users whose mentions should not be recorded
# Use the database to manage scan targets instead of .env variables.

# Runtime/config values
# Maximum number of retry attempts when fetching subreddit metadata from Reddit's /about.json endpoint
SUBABOUT_MAX_RETRIES=3
# Timeout in seconds for HTTP requests to Reddit API (prevents hanging on slow/failed connections)
HTTP_REQUEST_TIMEOUT=15

# How many hours before metadata is considered stale and needs refreshing
METADATA_STALE_HOURS=0

# Redis cache TTL (time-to-live) in seconds for API responses
# Controls how long API responses are cached before being refreshed
CACHE_TTL_DEFAULT=30       # General endpoints (subreddit listings, search results)
CACHE_TTL_STATS=60         # Statistics endpoints (counts, aggregations)
CACHE_TTL_ANALYTICS=300    # Analytics endpoints (5 minutes - complex queries, slower to change)

# Set to true, to add initial scan configuration on container start to the database.
# This is idempotent and safe to run multiple times.
INIT_SCAN_CONFIG=true

# Optional: Cloudflare Tunnel Token (if using cloudflared service in docker-compose)
# Generate from: https://one.dash.cloudflare.com/ -> Zero Trust -> Networks -> Tunnels
# CLOUDFLARE_TUNNEL_TOKEN=your_tunnel_token_here
